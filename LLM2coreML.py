#!/usr/bin/env python3
# é€šç”¨LLMåˆ°CoreMLè½¬æ¢å·¥å…·
# ä¸“ä¸ºAppleè®¾å¤‡ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹è½¬æ¢è„šæœ¬
# åªéœ€ä¿®æ”¹æ¨¡å‹IDå³å¯è½¬æ¢ä¸åŒçš„æ¨¡å‹

import os
import torch
import coremltools as ct
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse
import json
import time
import datetime
import sys
import subprocess
from pathlib import Path

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_CACHE_DIR = SCRIPT_DIR
DEFAULT_OUTPUT_DIR = os.path.join(SCRIPT_DIR, "models")

# æ”¯æŒçš„LLMæ¨¡å‹ç¤ºä¾‹
SUPPORTED_MODELS = {
    "phi": [
        "microsoft/phi-3-mini-4k-instruct",      # é»˜è®¤æ¨¡å‹ï¼Œæ¨èé¦–é€‰
        "microsoft/phi-3-mini-128k-instruct",    # ä¸Šä¸‹æ–‡çª—å£æ›´å¤§çš„ç‰ˆæœ¬
        "microsoft/phi-3-medium-4k-instruct",    # æ›´å¼ºå¤§çš„Phi-3 mediumç‰ˆæœ¬
    ],
    "gemma": [
        "google/gemma-2b",                       # è½»é‡çº§Gemma
        "google/gemma-7b",                       # å®Œæ•´ç‰ˆGemma
    ],
    "llama": [
        "meta-llama/Llama-2-7b-chat-hf",         # Llama 2èŠå¤©æ¨¡å‹
        "meta-llama/Llama-2-13b-chat-hf",        # æ›´å¤§çš„Llama 2èŠå¤©æ¨¡å‹
    ],
    "mistral": [
        "mistralai/Mistral-7B-Instruct-v0.2",    # MistralæŒ‡ä»¤æ¨¡å‹
    ],
    "qwen": [
        "Qwen/Qwen-1_8B-Chat",                   # é€šä¹‰åƒé—®è½»é‡ç‰ˆ
    ]
}

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–é¡¹"""
    print("æ­£åœ¨æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–é¡¹...")
    
    dependencies = [
        "torch>=2.0.0",
        "transformers>=4.35.0",
        "coremltools>=7.1",
        "numpy>=1.23.0"
    ]
    
    try:
        for dep in dependencies:
            print(f"æ­£åœ¨å®‰è£… {dep}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", dep])
        
        print("âœ… æ‰€æœ‰ä¾èµ–é¡¹å®‰è£…å®Œæˆ!")
        return True
    except Exception as e:
        print(f"âŒ å®‰è£…ä¾èµ–é¡¹æ—¶å‡ºé”™: {str(e)}")
        return False

def convert_llm_to_coreml(
    # ===== åªéœ€ä¿®æ”¹è¿™é‡Œçš„æ¨¡å‹IDï¼Œå³å¯è½¬æ¢ä¸åŒçš„LLMæ¨¡å‹ =====
    model_id="microsoft/phi-3-mini-4k-instruct",  # é»˜è®¤ä½¿ç”¨Phi-3-mini-4kæ¨¡å‹
    output_dir=None,
    max_seq_len=4096,
    compute_units=ct.ComputeUnit.CPU_ONLY,
    use_float16=False,
    cache_dir=None
):
    """
    å°†ä»»æ„LLMæ¨¡å‹è½¬æ¢ä¸ºCoreMLæ ¼å¼
    
    åªéœ€ä¿®æ”¹model_idå‚æ•°ï¼Œå³å¯ä¸€é”®è½¬æ¢ä¸åŒçš„æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
    - microsoft/phi-3-mini-4k-instruct (é»˜è®¤)
    - google/gemma-2b
    - meta-llama/Llama-2-7b-chat-hf
    - mistralai/Mistral-7B-Instruct-v0.2
    
    å‚æ•°:
        model_id (str): Hugging Faceæ¨¡å‹ID
        output_dir (str): è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"./models"
        max_seq_len (int): æœ€å¤§åºåˆ—é•¿åº¦
        compute_units: CoreMLè®¡ç®—å•å…ƒ (CPU_ONLY, CPU_AND_GPU, CPU_AND_NE, ...)
        use_float16 (bool): æ˜¯å¦ä½¿ç”¨float16ç²¾åº¦
        cache_dir (str): æ¨¡å‹ç¼“å­˜ç›®å½•
    
    è¿”å›:
        (bool, str): (æˆåŠŸçŠ¶æ€, æ¨¡å‹è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
    """
    try:
        start_time = time.time()
        
        # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
        if output_dir is None:
            model_name = model_id.split("/")[-1]
            output_dir = DEFAULT_OUTPUT_DIR
        
        # è®¾ç½®é»˜è®¤ç¼“å­˜ç›®å½•
        if cache_dir is None:
            cache_dir = DEFAULT_CACHE_DIR
        
        print(f"ğŸš€ å¼€å§‹è½¬æ¢æ¨¡å‹ {model_id} ä¸ºCoreMLæ ¼å¼")
        print(f"ğŸ“‹ è½¬æ¢é…ç½®:")
        print(f"   - æ¨¡å‹ID: {model_id}")
        print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
        print(f"   - æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_len}")
        print(f"   - è®¡ç®—å•å…ƒ: {compute_units}")
        print(f"   - ä½¿ç”¨float16: {use_float16}")
        print(f"   - ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨
        print("\nğŸ“š åŠ è½½åˆ†è¯å™¨...")
        tokenizer_kwargs = {
            "local_files_only": False,
            "trust_remote_code": True
        }
        if cache_dir:
            tokenizer_kwargs["cache_dir"] = cache_dir
            
        tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ§  åŠ è½½æ¨¡å‹...")
        model_kwargs = {
            "torch_dtype": torch.float16 if use_float16 else torch.float32,
            "trust_remote_code": True,
            "local_files_only": False
        }
        
        if cache_dir:
            model_kwargs["cache_dir"] = cache_dir
            
        model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)
        model.eval()
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        sample_text = "ä½ å¥½ï¼Œè¯·å‘Šè¯‰æˆ‘ä½ æ˜¯è°ã€‚"
        inputs = tokenizer(sample_text, return_tensors="pt")
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        
        print(f"ğŸ“Š ç¤ºä¾‹è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
        
        # ç®€åŒ–çš„æ¨¡å‹ç±»
        class SimpleModel(torch.nn.Module):
            def __init__(self, model):
                super().__init__()
                self.model = model
            
            def forward(self, input_ids, attention_mask):
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                return outputs.logits
        
        # åŒ…è£…æ¨¡å‹
        wrapped_model = SimpleModel(model)
        
        # ä½¿ç”¨JITè¿½è¸ª
        print("ğŸ” ä½¿ç”¨JITè¿½è¸ªæ¨¡å‹...")
        with torch.no_grad():
            traced_model = torch.jit.trace(
                wrapped_model,
                (input_ids, attention_mask)
            )
        
        # è¾“å…¥è§„æ ¼
        input_specs = [
            ct.TensorType(name="input_ids", shape=input_ids.shape, dtype=np.int32),
            ct.TensorType(name="attention_mask", shape=attention_mask.shape, dtype=np.int32)
        ]
        
        # è®¾ç½®è¾“å‡ºè·¯å¾„
        model_name = model_id.split("/")[-1]
        output_path = os.path.join(output_dir, f"{model_name}.mlpackage")
        
        # è½¬æ¢ä¸ºCoreML
        print("âš™ï¸ è½¬æ¢ä¸ºCoreMLæ ¼å¼...")
        mlmodel = ct.convert(
            traced_model,
            inputs=input_specs,
            compute_units=compute_units,
            minimum_deployment_target=ct.target.macOS13
        )
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜CoreMLæ¨¡å‹åˆ°: {output_path}")
        mlmodel.save(output_path)
        
        # è®¡ç®—è½¬æ¢æ—¶é—´
        end_time = time.time()
        conversion_time = end_time - start_time
        
        # ä¿å­˜æˆåŠŸä¿¡æ¯
        success_info = {
            "status": "success",
            "model_id": model_id,
            "output_path": output_path,
            "conversion_time_seconds": conversion_time,
            "conversion_time_formatted": str(datetime.timedelta(seconds=int(conversion_time))),
            "timestamp": datetime.datetime.now().isoformat()
        }
        
        success_path = os.path.join(output_dir, "conversion_success.json")
        with open(success_path, "w") as f:
            json.dump(success_info, f, indent=2)
        
        print(f"\nâœ… è½¬æ¢æˆåŠŸ! ç”¨æ—¶: {str(datetime.timedelta(seconds=int(conversion_time)))}")
        return True, output_path
    
    except Exception as e:
        error_message = str(e)
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {error_message}")
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        if output_dir:
            error_info = {
                "status": "error",
                "model_id": model_id,
                "error_message": error_message,
                "timestamp": datetime.datetime.now().isoformat()
            }
            
            error_path = os.path.join(output_dir, "conversion_error.json")
            with open(error_path, "w") as f:
                json.dump(error_info, f, indent=2)
        
        return False, error_message

def list_supported_models():
    """åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹ç¤ºä¾‹"""
    print("\nğŸ“‹ æ”¯æŒçš„LLMæ¨¡å‹ç¤ºä¾‹ï¼ˆå¯é€šè¿‡--model_idå‚æ•°ä½¿ç”¨ï¼‰:")
    
    for category, models in SUPPORTED_MODELS.items():
        print(f"\nğŸ“Œ {category.upper()} ç³»åˆ—:")
        for model in models:
            print(f"  - {model}")
    
    print("\nâ­ ä»¥åŠå…¶ä»– Hugging Face ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹")
    print("ğŸ”— æŸ¥çœ‹æ›´å¤šæ¨¡å‹: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n")

def main():
    parser = argparse.ArgumentParser(description="å°†ä»»æ„LLMæ¨¡å‹è½¬æ¢ä¸ºCoreMLæ ¼å¼ï¼Œç”¨äºAppleè®¾å¤‡")
    parser.add_argument("--model_id", type=str, default="microsoft/phi-3-mini-4k-instruct",
                      help="Hugging Faceæ¨¡å‹ID (é»˜è®¤: microsoft/phi-3-mini-4k-instruct)")
    parser.add_argument("--output_dir", type=str, default=None,
                      help=f"è¾“å‡ºç›®å½• (é»˜è®¤: {DEFAULT_OUTPUT_DIR})")
    parser.add_argument("--max_seq_len", type=int, default=4096,
                      help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 4096)")
    parser.add_argument("--use_float16", action="store_true",
                      help="ä½¿ç”¨float16ç²¾åº¦ (é»˜è®¤: False)")
    parser.add_argument("--use_gpu", action="store_true",
                      help="ä½¿ç”¨GPUè¿›è¡Œè½¬æ¢ (é»˜è®¤: ä»…ä½¿ç”¨CPU)")
    parser.add_argument("--cache_dir", type=str, default=None,
                      help=f"æ¨¡å‹ç¼“å­˜ç›®å½• (é»˜è®¤: {DEFAULT_CACHE_DIR})")
    parser.add_argument("--install_deps", action="store_true",
                      help="å®‰è£…ä¾èµ–é¡¹ (é»˜è®¤: False)")
    parser.add_argument("--list_models", action="store_true",
                      help="åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹ç¤ºä¾‹ (é»˜è®¤: False)")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹
    if args.list_models:
        list_supported_models()
        return
    
    # å®‰è£…ä¾èµ–é¡¹
    if args.install_deps:
        if not install_dependencies():
            print("ä¾èµ–é¡¹å®‰è£…å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            sys.exit(1)
    
    # è®¾ç½®è®¡ç®—å•å…ƒ
    compute_units = ct.ComputeUnit.CPU_AND_GPU if args.use_gpu else ct.ComputeUnit.CPU_ONLY
    
    success, result = convert_llm_to_coreml(
        model_id=args.model_id,
        output_dir=args.output_dir,
        max_seq_len=args.max_seq_len,
        compute_units=compute_units,
        use_float16=args.use_float16,
        cache_dir=args.cache_dir
    )
    
    if success:
        print(f"\nğŸ‰ è½¬æ¢æˆåŠŸ! æ¨¡å‹å·²ä¿å­˜è‡³: {result}")
        print("ğŸ ç°åœ¨å¯ä»¥åœ¨Appleè®¾å¤‡ä¸Šä½¿ç”¨æ­¤CoreMLæ¨¡å‹")
        sys.exit(0)
    else:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {result}")
        print("ğŸ’¡ æç¤º: å°è¯•ä½¿ç”¨--use_float16å‚æ•°å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œæˆ–é€‰æ‹©æ›´å°çš„æ¨¡å‹")
        sys.exit(1)

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ LLM2CoreML è½¬æ¢å·¥å…· - è®©å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Appleè®¾å¤‡ä¸Šè¿è¡Œ")
    print("=" * 80)
    main() 